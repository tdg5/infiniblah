0% [Working]            Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]
0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [1 In0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Conn                                                                               Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
0% [Connecting to archive.ubuntu.com] [2 InRelease 0 B/114 kB 0%] [Connecting t0% [Connecting to archive.ubuntu.com] [Connecting to ppa.launchpad.net] [Connec0% [Connecting to archive.ubuntu.com] [Connecting to ppa.launchpad.net] [Waitin                                                                               Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]
0% [3 InRelease 0 B/265 kB 0%] [Waiting for headers] [Connecting to ppa.launchp                                                                               Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,423 kB]
0% [3 InRelease 125 kB/265 kB 47%] [4 Packages 12.3 kB/1,423 kB 1%] [Connecting0% [4 Packages 98.3 kB/1,423 kB 7%] [Connecting to ppa.launchpad.net] [Waiting                                                                                Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
0% [5 InRelease 0 B/114 kB 0%] [4 Packages 164 kB/1,423 kB 12%] [Connecting to 0% [4 Packages 325 kB/1,423 kB 23%] [Connecting to ppa.launchpad.net] [Waiting                                                                                Get:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
0% [6 InRelease 0 B/108 kB 0%] [4 Packages 475 kB/1,423 kB 33%] [Connecting to 0% [4 Packages 653 kB/1,423 kB 46%] [Connecting to ppa.launchpad.net] [Waiting                                                                                0% [Connecting to ppa.launchpad.net (185.125.190.80)] [Waiting for headers]0% [4 Packages store 0 B] [Connecting to ppa.launchpad.net (185.125.190.80)] [W                                                                               Get:7 https://deb.nodesource.com/node_20.x nodistro InRelease [12.1 kB]
0% [4 Packages store 0 B] [Connecting to ppa.launchpad.net (185.125.190.80)] [7                                                                               0% [4 Packages store 0 B] [Connecting to ppa.launchpad.net (185.125.190.80)]                                                                            0% [Waiting for headers]0% [Waiting for headers]                        Get:8 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3,421 kB]
0% [8 Packages 12.3 kB/3,421 kB 0%] [Waiting for headers]                                                         Get:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]
0% [8 Packages 2,556 kB/3,421 kB 75%] [9 InRelease 13.7 kB/18.1 kB 76%]                                                                       0% [9 InRelease 13.7 kB/18.1 kB 76%]0% [8 Packages store 0 B] [9 InRelease 13.7 kB/18.1 kB 76%]                                                           Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,187 kB]
0% [8 Packages store 0 B] [10 Packages 0 B/1,187 kB 0%] [9 InRelease 13.7 kB/180% [8 Packages store 0 B] [Waiting for headers] [9 InRelease 13.7 kB/18.1 kB 76                                                                               Get:11 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3,327 kB]
0% [8 Packages store 0 B] [11 Packages 61.5 kB/3,327 kB 2%] [9 InRelease 13.7 k0% [8 Packages store 0 B] [Waiting for headers] [9 InRelease 13.7 kB/18.1 kB 76                                                                               Get:12 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.7 kB]
0% [8 Packages store 0 B] [12 Packages 28.7 kB/29.7 kB 97%] [9 InRelease 13.7 k                                                                               0% [8 Packages store 0 B] [9 InRelease 13.7 kB/18.1 kB 76%]                                                           0% [8 Packages store 0 B]0% [8 Packages store 0 B] [Waiting for headers]                                               Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1,275 kB]
0% [8 Packages store 0 B] [13 Packages 0 B/1,275 kB 0%] [Waiting for headers]                                                                             0% [8 Packages store 0 B] [Waiting for headers]                                               Get:14 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]
0% [8 Packages store 0 B] [14 Packages 6,159 B/177 kB 3%] [Waiting for headers]                                                                               0% [8 Packages store 0 B] [Waiting for headers] [Waiting for headers]                                                                     Get:15 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]
0% [8 Packages store 0 B] [15 Packages 0 B/33.4 kB 0%] [Waiting for headers]                                                                            0% [8 Packages store 0 B] [Waiting for headers] [Waiting for headers]                                                                     Get:16 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]
0% [8 Packages store 0 B] [16 Packages 32.8 kB/11.3 MB 0%] [Waiting for headers                                                                               Get:17 http://ppa.launchpad.net/git-core/ppa/ubuntu focal InRelease [23.8 kB]
0% [8 Packages store 0 B] [16 Packages 3,072 kB/11.3 MB 27%] [17 InRelease 0 B/                                                                               0% [8 Packages store 0 B] [16 Packages 3,138 kB/11.3 MB 28%]                                                            0% [16 Packages 5,890 kB/11.3 MB 52%]0% [10 Packages store 0 B] [16 Packages 5,898 kB/11.3 MB 52%]0% [10 Packages store 0 B] [16 Packages 9,560 kB/11.3 MB 84%]                                                             0% [10 Packages store 0 B]                          Get:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3,477 kB]
0% [10 Packages store 0 B] [18 Packages 50.5 kB/3,477 kB 1%]                                                            0% [18 Packages 1,638 kB/3,477 kB 47%]0% [11 Packages store 0 B] [18 Packages 1,638 kB/3,477 kB 47%]                                                              Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,895 kB]
                                                              0% [11 Packages store 0 B] [19 Packages 46.5 kB/3,895 kB 1%]                                                            Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,483 kB]
0% [11 Packages store 0 B] [20 Packages 24.6 kB/1,483 kB 2%]                                                            0% [11 Packages store 0 B] [Waiting for headers]                                                Get:21 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.4 kB]
0% [11 Packages store 0 B] [21 Packages 32.4 kB/32.4 kB 100%]                                                             0% [11 Packages store 0 B]0% [11 Packages store 0 B]                          Get:22 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]
0% [11 Packages store 0 B] [22 Packages 8,192 B/55.2 kB 15%]                                                            0% [11 Packages store 0 B]                          Get:23 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]
0% [11 Packages store 0 B] [23 Packages 16.4 kB/28.6 kB 57%]                                                            0% [11 Packages store 0 B]0% [11 Packages store 0 B]                          0% [Waiting for headers]0% [12 Packages store 0 B] [Waiting for headers]                                                0% [Waiting for headers]0% [13 Packages store 0 B] [Waiting for headers]                                                Get:24 https://deb.nodesource.com/node_20.x nodistro/main amd64 Packages [5,184 B]
0% [13 Packages store 0 B] [24 Packages 5,184 B/5,184 B 100%]                                                             0% [13 Packages store 0 B]                          0% [Working]0% [14 Packages store 0 B]                          0% [Working]0% [15 Packages store 0 B]                          0% [Working]0% [16 Packages store 0 B]0% [16 Packages store 0 B]                          Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [35.5 kB]
0% [16 Packages store 0 B] [25 Packages 22.9 kB/35.5 kB 64%]96% [16 Packages store 0 B] [25 Packages 25.0 kB/35.5 kB 70%]                                                             96% [16 Packages store 0 B]                           Get:26 http://ppa.launchpad.net/git-core/ppa/ubuntu focal/main amd64 Packages [3,176 B]
96% [16 Packages store 0 B] [26 Packages 3,176 B/3,176 B 100%]                                                              96% [16 Packages store 0 B]                           96% [Working]96% [18 Packages store 0 B]                           97% [Working]97% [19 Packages store 0 B]                           97% [Working]97% [20 Packages store 0 B]                           97% [Working]97% [21 Packages store 0 B]                           98% [Working]98% [22 Packages store 0 B]                           98% [Working]98% [23 Packages store 0 B]                           99% [Working]99% [24 Packages store 0 B]                           99% [Working]99% [25 Packages store 0 B]                           100% [Working]100% [26 Packages store 0 B]                            100% [Working]              Fetched 31.9 MB in 2s (20.2 MB/s)
Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 39%Reading package lists... 39%Reading package lists... 40%Reading package lists... 40%Reading package lists... 52%Reading package lists... 52%Reading package lists... 63%Reading package lists... 63%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 79%Reading package lists... 79%Reading package lists... 90%Reading package lists... 90%Reading package lists... 94%Reading package lists... 94%Reading package lists... 94%Reading package lists... 94%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
Reading package lists... 0%Reading package lists... 0%Reading package lists... 0%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 4%Reading package lists... 39%Reading package lists... 39%Reading package lists... 40%Reading package lists... 40%Reading package lists... 52%Reading package lists... 52%Reading package lists... 63%Reading package lists... 63%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 68%Reading package lists... 79%Reading package lists... 79%Reading package lists... 90%Reading package lists... 90%Reading package lists... 94%Reading package lists... 94%Reading package lists... 94%Reading package lists... 94%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... 99%Reading package lists... Done
Building dependency tree... 0%Building dependency tree... 0%Building dependency tree... 36%Building dependency tree... 50%Building dependency tree... 50%Building dependency tree       
Reading state information... 0%Reading state information... 0%Reading state information... Done
The following additional packages will be installed:
  libatm1 libpcap0.8 libpopt0 libxtables12
Suggested packages:
  iproute2-doc
The following NEW packages will be installed:
  iftop iproute2 libatm1 libpcap0.8 libpopt0 libxtables12 nethogs rsync
0 upgraded, 8 newly installed, 0 to remove and 41 not upgraded.
Need to get 1,451 kB of archives.
After this operation, 4,370 kB of additional disk space will be used.
0% [Working]            Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libpopt0 amd64 1.16-14 [26.3 kB]
0% [1 libpopt0 0 B/26.3 kB 0%]                              4% [Working]            Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 rsync amd64 3.1.3-8ubuntu0.7 [322 kB]
4% [2 rsync 0 B/322 kB 0%]                          24% [Waiting for headers]                         Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libxtables12 amd64 1.8.4-3ubuntu2.1 [28.7 kB]
26% [3 libxtables12 28.7 kB/28.7 kB 100%]                                         Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 iproute2 amd64 5.5.0-1ubuntu1 [858 kB]
                                         32% [4 iproute2 65.5 kB/858 kB 8%]                                  78% [Waiting for headers]                         Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libatm1 amd64 1:2.5.1-4 [21.8 kB]
79% [5 libatm1 21.8 kB/21.8 kB 100%]                                    82% [Waiting for headers]                         Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libpcap0.8 amd64 1.9.1-3 [128 kB]
82% [6 libpcap0.8 8,192 B/128 kB 6%]                                    91% [Waiting for headers]                         Get:7 http://archive.ubuntu.com/ubuntu focal/universe amd64 iftop amd64 1.0~pre4-6build1 [36.3 kB]
93% [7 iftop 36.3 kB/36.3 kB 100%]                                  96% [Waiting for headers]                         Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 nethogs amd64 0.8.5-2build2 [29.9 kB]
97% [8 nethogs 18.5 kB/29.9 kB 62%]                                   100% [Working]              Fetched 1,451 kB in 0s (52.5 MB/s)
Preconfiguring packages ...
Selecting previously unselected package libpopt0:amd64.
(Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 48435 files and directories currently installed.)
Preparing to unpack .../0-libpopt0_1.16-14_amd64.deb ...
Unpacking libpopt0:amd64 (1.16-14) ...
Selecting previously unselected package rsync.
Preparing to unpack .../1-rsync_3.1.3-8ubuntu0.7_amd64.deb ...
Unpacking rsync (3.1.3-8ubuntu0.7) ...
Selecting previously unselected package libxtables12:amd64.
Preparing to unpack .../2-libxtables12_1.8.4-3ubuntu2.1_amd64.deb ...
Unpacking libxtables12:amd64 (1.8.4-3ubuntu2.1) ...
Selecting previously unselected package iproute2.
Preparing to unpack .../3-iproute2_5.5.0-1ubuntu1_amd64.deb ...
Unpacking iproute2 (5.5.0-1ubuntu1) ...
Selecting previously unselected package libatm1:amd64.
Preparing to unpack .../4-libatm1_1%3a2.5.1-4_amd64.deb ...
Unpacking libatm1:amd64 (1:2.5.1-4) ...
Selecting previously unselected package libpcap0.8:amd64.
Preparing to unpack .../5-libpcap0.8_1.9.1-3_amd64.deb ...
Unpacking libpcap0.8:amd64 (1.9.1-3) ...
Selecting previously unselected package iftop.
Preparing to unpack .../6-iftop_1.0~pre4-6build1_amd64.deb ...
Unpacking iftop (1.0~pre4-6build1) ...
Selecting previously unselected package nethogs.
Preparing to unpack .../7-nethogs_0.8.5-2build2_amd64.deb ...
Unpacking nethogs (0.8.5-2build2) ...
Setting up libatm1:amd64 (1:2.5.1-4) ...
Setting up libpcap0.8:amd64 (1.9.1-3) ...
Setting up libxtables12:amd64 (1.8.4-3ubuntu2.1) ...
Setting up libpopt0:amd64 (1.16-14) ...
Setting up nethogs (0.8.5-2build2) ...
Setting up iproute2 (5.5.0-1ubuntu1) ...
Setting up iftop (1.0~pre4-6build1) ...
Setting up rsync (3.1.3-8ubuntu0.7) ...
invoke-rc.d: could not determine current runlevel
invoke-rc.d: policy-rc.d denied execution of start.
Created symlink /etc/systemd/system/multi-user.target.wants/rsync.service â†’ /lib/systemd/system/rsync.service.
Processing triggers for systemd (245.4-4ubuntu3.23) ...
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for libc-bin (2.31-0ubuntu9.14) ...
sending incremental file list
created directory /root/c4
./
train_small/
train_small/index.json
train_small/shard.00000.mds
train_small/shard.00001.mds
train_small/shard.00002.mds
train_small/shard.00003.mds
train_small/shard.00004.mds
train_small/shard.00005.mds
train_small/shard.00006.mds
train_small/shard.00007.mds
train_small/shard.00008.mds
train_small/shard.00009.mds
train_small/shard.00010.mds
train_small/shard.00011.mds
train_small/shard.00012.mds
train_small/shard.00013.mds
train_small/shard.00014.mds
train_small/shard.00015.mds
train_small/shard.00016.mds
train_small/shard.00017.mds
train_small/shard.00018.mds
train_small/shard.00019.mds
train_small/shard.00020.mds
train_small/shard.00021.mds
train_small/shard.00022.mds
train_small/shard.00023.mds
train_small/shard.00024.mds
val_small/
val_small/index.json
val_small/shard.00000.mds
val_small/shard.00001.mds
val_small/shard.00002.mds

sent 1,803,575,389 bytes  received 636 bytes  400,794,672.22 bytes/sec
total size is 1,803,133,184  speedup is 1.00
CA 'mlx5_0'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027ddde
	System image GUID: 0xb83fd2030027ddde
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 1
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e84a
		Port GUID: 0xb83fd2030027ddde
		Link layer: InfiniBand
CA 'mlx5_1'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027ed76
	System image GUID: 0xb83fd2030027ed76
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 33
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027ed76
		Link layer: InfiniBand
CA 'mlx5_2'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027caba
	System image GUID: 0xb83fd2030027caba
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 54
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027caba
		Link layer: InfiniBand
CA 'mlx5_3'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027dd8e
	System image GUID: 0xb83fd2030027dd8e
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 39
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027dd8e
		Link layer: InfiniBand
CA 'mlx5_4'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027ed72
	System image GUID: 0xb83fd2030027ed72
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 29
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027ed72
		Link layer: InfiniBand
CA 'mlx5_5'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027ba26
	System image GUID: 0xb83fd2030027ba26
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 8
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027ba26
		Link layer: InfiniBand
CA 'mlx5_6'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027ed5a
	System image GUID: 0xb83fd2030027ed5a
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 19
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027ed5a
		Link layer: InfiniBand
CA 'mlx5_7'
	CA type: MT4123
	Number of ports: 1
	Firmware version: 20.40.1000
	Hardware version: 0
	Node GUID: 0xb83fd2030027eac6
	System image GUID: 0xb83fd2030027eac6
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 58
		LMC: 0
		SM lid: 16
		Capability mask: 0xa651e848
		Port GUID: 0xb83fd2030027eac6
		Link layer: InfiniBand
CA: workernode13 mlx5_4:
      0xb83fd2030027ed72     29    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   32[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: Mellanox Technologies Aggregation Node:
      0x946dae0300be83aa     67    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   41[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode13 mlx5_7:
      0xb83fd2030027eac6     58    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   31[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode13 mlx5_5:
      0xb83fd2030027ba26      8    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   30[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode13 mlx5_0:
      0xb83fd2030027ddde      1    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   27[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode13 mlx5_1:
      0xb83fd2030027ed76     33    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   28[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode13 mlx5_3:
      0xb83fd2030027dd8e     39    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   26[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode13 mlx5_2:
      0xb83fd2030027caba     54    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   25[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_4:
      0xb83fd2030027a7fe     65    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   24[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_5:
      0xb83fd2030027a866     23    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   23[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_7:
      0xb83fd2030027a86a     25    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   22[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_6:
      0xb83fd2030027a862     21    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   21[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_1:
      0xb83fd2030027a856     17    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   20[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_0:
      0xb83fd2030027a852     16    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   19[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_3:
      0xb83fd2030027a69e     44    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   18[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode14 mlx5_2:
      0xb83fd2030027a84e     14    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   17[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_4:
      0xb83fd2030027ba0e      5    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   16[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_5:
      0xb83fd2030027a30a      3    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   15[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_7:
      0xb83fd2030027ed66     24    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   14[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_6:
      0xb83fd2030027ed8e     40    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   13[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_0:
      0xb83fd2030027ddaa     50    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   11[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_1:
      0xb83fd2030027ed6e     26    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   12[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_3:
      0xb83fd2030027a85e     20    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   10[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode15 mlx5_2:
      0xb83fd2030027a846     11    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    9[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_4:
      0xb83fd2030027ca0e      6    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    8[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_5:
      0xb83fd2030027cace     60    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    7[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_7:
      0xb83fd2030027cafa     64    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    6[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_6:
      0xb83fd2030027cae6     61    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    5[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_1:
      0xb83fd2030027a90e      4    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    4[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_0:
      0xb83fd2030027ba3a      9    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    3[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_3:
      0xb83fd2030027ed7a     34    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    2[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
CA: workernode16 mlx5_2:
      0xb83fd2030027a912      7    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45    1[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
Switch: 0x946dae0300be83a2 MF0;sw-gpu13to16:MQM8700/U1:
          45    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       7    1[  ] "workernode16 mlx5_2" ( )
          45    2[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      34    1[  ] "workernode16 mlx5_3" ( )
          45    3[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       9    1[  ] "workernode16 mlx5_0" ( )
          45    4[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       4    1[  ] "workernode16 mlx5_1" ( )
          45    5[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      61    1[  ] "workernode16 mlx5_6" ( )
          45    6[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      64    1[  ] "workernode16 mlx5_7" ( )
          45    7[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      60    1[  ] "workernode16 mlx5_5" ( )
          45    8[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       6    1[  ] "workernode16 mlx5_4" ( )
          45    9[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      11    1[  ] "workernode15 mlx5_2" ( )
          45   10[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      20    1[  ] "workernode15 mlx5_3" ( )
          45   11[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      50    1[  ] "workernode15 mlx5_0" ( )
          45   12[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      26    1[  ] "workernode15 mlx5_1" ( )
          45   13[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      40    1[  ] "workernode15 mlx5_6" ( )
          45   14[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      24    1[  ] "workernode15 mlx5_7" ( )
          45   15[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       3    1[  ] "workernode15 mlx5_5" ( )
          45   16[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       5    1[  ] "workernode15 mlx5_4" ( )
          45   17[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      14    1[  ] "workernode14 mlx5_2" ( )
          45   18[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      44    1[  ] "workernode14 mlx5_3" ( )
          45   19[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      16    1[  ] "workernode14 mlx5_0" ( )
          45   20[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      17    1[  ] "workernode14 mlx5_1" ( )
          45   21[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      21    1[  ] "workernode14 mlx5_6" ( )
          45   22[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      25    1[  ] "workernode14 mlx5_7" ( )
          45   23[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      23    1[  ] "workernode14 mlx5_5" ( )
          45   24[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      65    1[  ] "workernode14 mlx5_4" ( )
          45   25[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      54    1[  ] "workernode13 mlx5_2" ( )
          45   26[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      39    1[  ] "workernode13 mlx5_3" ( )
          45   27[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       1    1[  ] "workernode13 mlx5_0" ( )
          45   28[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      33    1[  ] "workernode13 mlx5_1" ( )
          45   29[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      19    1[  ] "workernode13 mlx5_6" ( )
          45   30[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>       8    1[  ] "workernode13 mlx5_5" ( )
          45   31[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      58    1[  ] "workernode13 mlx5_7" ( )
          45   32[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      29    1[  ] "workernode13 mlx5_4" ( )
          45   33[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   34[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   35[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   36[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   37[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   38[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   39[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   40[  ] ==(                Down/ Polling)==>             [  ] "" ( )
          45   41[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      67    1[  ] "Mellanox Technologies Aggregation Node" ( )
CA: workernode13 mlx5_6:
      0xb83fd2030027ed5a     19    1[  ] ==( 4X        53.125 Gbps Active/  LinkUp)==>      45   29[  ] "MF0;sw-gpu13to16:MQM8700/U1" ( )
[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)
/root/github/llm-foundry/scripts/train/train.py:375: UserWarning: Unused parameter device_eval_microbatch_size found in cfg. Please check your yaml to ensure this parameter is necessary.
  warnings.warn(
2024-03-06 03:36:49,640: rank0[2748730][MainThread]: INFO: __main__: Building tokenizer...
tokenizer_config.json:   0% 0.00/156 [00:00<?, ?B/s]tokenizer_config.json: 100% 156/156 [00:00<00:00, 772kB/s]
vocab.json:   0% 0.00/1.08M [00:00<?, ?B/s]vocab.json: 100% 1.08M/1.08M [00:00<00:00, 18.7MB/s]
merges.txt:   0% 0.00/457k [00:00<?, ?B/s]merges.txt: 100% 457k/457k [00:00<00:00, 57.5MB/s]
tokenizer.json:   0% 0.00/2.11M [00:00<?, ?B/s]tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 20.7MB/s]
special_tokens_map.json:   0% 0.00/90.0 [00:00<?, ?B/s]special_tokens_map.json: 100% 90.0/90.0 [00:00<00:00, 505kB/s]
2024-03-06 03:36:56,845: rank0[2748730][MainThread]: INFO: __main__: Building train loader...
2024-03-06 03:36:56,847: rank0[2748730][MainThread]: WARNING: streaming.base.dataset: Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).
2024-03-06 03:36:57,176: rank0[2748730][MainThread]: INFO: __main__: Building eval loader...
2024-03-06 03:36:57,177: rank0[2748730][MainThread]: WARNING: streaming.base.dataset: Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).
2024-03-06 03:36:57,300: rank0[2748730][MainThread]: INFO: __main__: Initializing model...
/root/github/llm-foundry/llmfoundry/models/mpt/configuration_mpt.py:231: UserWarning: If not using a Prefix Language Model, we recommend setting "attn_impl" to "flash" instead of "triton".
  warnings.warn(
2024-03-06 03:36:57,302: rank0[2748730][MainThread]: INFO: llmfoundry.models.mpt.modeling_mpt: Instantiating an MPTForCausalLM model from /root/github/llm-foundry/llmfoundry/models/mpt/modeling_mpt.py
2024-03-06 03:36:57,334: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: MPTModel(
  (wte): SharedEmbedding(50368, 4096)
  (wpe): Embedding(2048, 4096)
  (emb_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0-31): 32 x MPTBlock(
      (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (Wqkv): Linear(in_features=4096, out_features=12288, bias=True)
        (out_proj): Linear(in_features=4096, out_features=4096, bias=True)
      )
      (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (ffn): MPTMLP(
        (up_proj): Linear(in_features=4096, out_features=16384, bias=True)
        (down_proj): Linear(in_features=16384, out_features=4096, bias=True)
      )
      (resid_attn_dropout): Dropout(p=0.0, inplace=False)
      (resid_ffn_dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)
2024-03-06 03:36:57,335: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: Using kaiming_normal_ initialization.
2024-03-06 03:36:57,467: rank0[2748730][MainThread]: INFO: __main__: Building trainer...
2024-03-06 03:36:57,469: rank0[2748730][MainThread]: INFO: composer.utils.reproducibility: Setting seed to 17
2024-03-06 03:36:57,469: rank0[2748730][MainThread]: INFO: composer.trainer.trainer: Run name: llm
2024-03-06 03:36:58,673: rank0[2748730][MainThread]: INFO: composer.trainer.trainer: Stepping schedulers every batch. To step schedulers every epoch, set `step_schedulers_every_batch=False`.
2024-03-06 03:36:59,015: rank0[2748730][MainThread]: INFO: composer.utils.reproducibility: Setting seed to 17
2024-03-06 03:36:59,200: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: SharedEmbedding cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,201: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: Embedding cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,201: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: Dropout cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,209: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: ModuleList cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,209: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: LPLayerNorm cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,209: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: MPTModel cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,209: rank0[2748730][MainThread]: DEBUG: llmfoundry.models.mpt.modeling_mpt: MPTForCausalLM cannot be activation checkpointed. Only transformer block or its submodules are eligible for activation checkpointing.
2024-03-06 03:36:59,211: rank0[2748730][MainThread]: DEBUG: composer.utils.reproducibility: Restoring the RNG state
2024-03-06 03:36:59,211: rank0[2748730][MainThread]: INFO: composer.trainer.trainer: Setting seed to 17
2024-03-06 03:36:59,211: rank0[2748730][MainThread]: INFO: composer.utils.reproducibility: Setting seed to 17
2024-03-06 03:36:59,211: rank0[2748730][MainThread]: INFO: __main__: Logging config
data_local: /root/c4
data_remote: null
max_seq_len: 2048
global_seed: 17
run_name: null
model:
  name: mpt_causal_lm
  init_device: meta
  d_model: 4096
  n_heads: 32
  n_layers: 32
  expansion_ratio: 4
  max_seq_len: 2048
  vocab_size: 50368
  attn_config:
    attn_impl: triton
tokenizer:
  name: EleutherAI/gpt-neox-20b
  kwargs:
    model_max_length: 2048
train_loader:
  name: text
  dataset:
    local: /root/c4
    remote: null
    split: train_small
    shuffle: true
    max_seq_len: 2048
    shuffle_seed: 17
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: /root/c4
    remote: null
    split: val_small
    shuffle: false
    max_seq_len: 2048
    shuffle_seed: 17
  drop_last: false
  num_workers: 8
scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  alpha_f: 0.1
optimizer:
  name: decoupled_adamw
  lr: 0.00012
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
  weight_decay: 0.0
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
max_duration: 100ba
eval_interval: 1ep
eval_first: false
eval_subset_num_batches: -1
global_train_batch_size: 256
seed: 17
device_eval_batch_size: 8
device_train_microbatch_size: 16
precision: amp_bf16
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: PURE
  activation_checkpointing: true
  activation_checkpointing_reentrant: false
  activation_cpu_offload: false
  limit_all_gathers: true
progress_bar: false
log_to_console: true
console_log_interval: 1ba
callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  runtime_estimator: {}
device_eval_microbatch_size: 16
n_gpus: 16
device_train_batch_size: 16
device_train_grad_accum: 1
merge: true
n_params: 6658859008
n_trainable_params: 6658859008

2024-03-06 03:36:59,396: rank0[2748730][MainThread]: INFO: __main__: Starting training...
2024-03-06 03:36:59,396: rank0[2748730][MainThread]: INFO: composer.trainer.trainer: Using precision Precision.AMP_BF16
******************************
Config:
composer_commit_hash: None
composer_version: 0.19.1
enabled_algorithms/GradientClipping: true
node_name: unknown because NODENAME environment variable not set
num_gpus_per_node: 8
num_nodes: 2
rank_zero_seed: 17
time/remaining_estimate_unit: hours

******************************
2024-03-06 03:36:59,397: rank0[2748730][MainThread]: DEBUG: composer.trainer.trainer: Spinning the dataloaders
2024-03-06 03:36:59,623: rank0[2751005][MainThread]: WARNING: streaming.base.dataset: Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes. Prior to Streaming v0.7.0, `num_canonical_nodes` defaulted to 64 * physical nodes.
2024-03-06 03:36:59,624: rank0[2751005][MainThread]: WARNING: streaming.base.dataset: Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144. Prior to Streaming v0.7.0, `shuffle_block_size` defaulted to 262144.
2024-03-06 03:37:00,798: rank0[2755785][MainThread]: WARNING: streaming.base.dataset: Because `num_canonical_nodes` was not specified, and `shuffle_algo` is py1e, it will default to be equal to physical nodes. Prior to Streaming v0.7.0, `num_canonical_nodes` defaulted to 64 * physical nodes.
2024-03-06 03:37:00,798: rank0[2755785][MainThread]: WARNING: streaming.base.dataset: Because `shuffle_block_size` was not specified, it will default to max(4_000_000 // num_canonical_nodes, 1 << 18) if num_canonical_nodes is not None, otherwise 262144. Prior to Streaming v0.7.0, `shuffle_block_size` defaulted to 262144.
[batch=1/100]:
	 Train time/epoch: 0
	 Train time/batch: 0
	 Train time/sample: 0
	 Train time/batch_in_epoch: 0
	 Train time/sample_in_epoch: 0
	 Train time/token: 0
	 Train time/token_in_epoch: 0
	 Train memory/current_allocated_mem: 8.8058
	 Train memory/current_active_mem: 8.8058
	 Train memory/current_inactive_mem: 3.9344
	 Train memory/current_reserved_mem: 34.7440
	 Train memory/peak_allocated_mem: 22.2490
	 Train memory/peak_active_mem: 23.2900
	 Train memory/peak_inactive_mem: 5.4147
	 Train memory/peak_reserved_mem: 34.7440
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 11.8596
	 Train metrics/train/LanguageCrossEntropy: 11.8596
	 Train metrics/train/LanguagePerplexity: 141435.9062
	 Train time/train: 0.0060
	 Train time/val: 0.0000
	 Train time/total: 0.0060
	 Train lr-DecoupledAdamW/group0: 0.0000
[batch=2/100]:
	 Train time/batch: 1
	 Train time/sample: 256
	 Train time/batch_in_epoch: 1
	 Train time/sample_in_epoch: 256
	 Train time/token: 524288
	 Train time/token_in_epoch: 524288
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 6.5920
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 9.6993
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 11.8526
	 Train metrics/train/LanguageCrossEntropy: 11.8526
	 Train metrics/train/LanguagePerplexity: 140451.5938
	 Train time/train: 0.0100
	 Train time/val: 0.0000
	 Train time/total: 0.0100
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3881
[batch=3/100]:
	 Train time/batch: 2
	 Train time/sample: 512
	 Train time/batch_in_epoch: 2
	 Train time/sample_in_epoch: 512
	 Train time/token: 1048576
	 Train time/token_in_epoch: 1048576
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 3.2911
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 11.8595
	 Train metrics/train/LanguageCrossEntropy: 11.8595
	 Train metrics/train/LanguagePerplexity: 141423.6406
	 Train time/train: 0.0139
	 Train time/val: 0.0000
	 Train time/total: 0.0139
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3844
[batch=4/100]:
	 Train time/batch: 3
	 Train time/sample: 768
	 Train time/batch_in_epoch: 3
	 Train time/sample_in_epoch: 768
	 Train time/token: 1572864
	 Train time/token_in_epoch: 1572864
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 6.5920
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 15.6870
	 Train metrics/train/LanguageCrossEntropy: 15.6868
	 Train metrics/train/LanguagePerplexity: 6496916.0000
	 Train time/train: 0.0179
	 Train time/val: 0.0000
	 Train time/total: 0.0179
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3803
[batch=5/100]:
	 Train time/batch: 4
	 Train time/sample: 1024
	 Train time/batch_in_epoch: 4
	 Train time/sample_in_epoch: 1024
	 Train time/token: 2097152
	 Train time/token_in_epoch: 2097152
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 4.3648
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 17.4698
	 Train metrics/train/LanguageCrossEntropy: 17.4695
	 Train metrics/train/LanguagePerplexity: 38626904.0000
	 Train time/train: 0.0217
	 Train time/val: 0.0000
	 Train time/total: 0.0217
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3741
[batch=6/100]:
	 Train time/batch: 5
	 Train time/sample: 1280
	 Train time/batch_in_epoch: 5
	 Train time/sample_in_epoch: 1280
	 Train time/token: 2621440
	 Train time/token_in_epoch: 2621440
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 7.6657
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 16.8042
	 Train metrics/train/LanguageCrossEntropy: 16.8039
	 Train metrics/train/LanguagePerplexity: 19853942.0000
	 Train time/train: 0.0256
	 Train time/val: 0.0000
	 Train time/total: 0.0256
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3690
[batch=7/100]:
	 Train time/batch: 6
	 Train time/sample: 1536
	 Train time/batch_in_epoch: 6
	 Train time/sample_in_epoch: 1536
	 Train time/token: 3145728
	 Train time/token_in_epoch: 3145728
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 3.2911
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 14.7055
	 Train metrics/train/LanguageCrossEntropy: 14.7055
	 Train metrics/train/LanguagePerplexity: 2434998.7500
	 Train time/train: 0.0297
	 Train time/val: 0.0000
	 Train time/total: 0.0297
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3675
[batch=8/100]:
	 Train time/batch: 7
	 Train time/sample: 1792
	 Train time/batch_in_epoch: 7
	 Train time/sample_in_epoch: 1792
	 Train time/token: 3670016
	 Train time/token_in_epoch: 3670016
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 6.5920
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 13.7454
	 Train metrics/train/LanguageCrossEntropy: 13.7454
	 Train metrics/train/LanguagePerplexity: 932245.7500
	 Train time/train: 0.0337
	 Train time/val: 0.0000
	 Train time/total: 0.0337
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3642
[batch=9/100]:
	 Train time/batch: 8
	 Train time/sample: 2048
	 Train time/batch_in_epoch: 8
	 Train time/sample_in_epoch: 2048
	 Train time/token: 4194304
	 Train time/token_in_epoch: 4194304
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 3.2911
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 12.3577
	 Train metrics/train/LanguageCrossEntropy: 12.3571
	 Train metrics/train/LanguagePerplexity: 232610.6875
	 Train time/train: 0.0377
	 Train time/val: 0.0000
	 Train time/total: 0.0377
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3606
[batch=10/100]:
	 Train time/batch: 9
	 Train time/sample: 2304
	 Train time/batch_in_epoch: 9
	 Train time/sample_in_epoch: 2304
	 Train time/token: 4718592
	 Train time/token_in_epoch: 4718592
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 7.6657
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 12.1679
	 Train metrics/train/LanguageCrossEntropy: 12.1661
	 Train metrics/train/LanguagePerplexity: 192162.8906
	 Train time/train: 0.0416
	 Train time/val: 0.0000
	 Train time/total: 0.0416
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3562
[batch=11/100]:
	 Train time/batch: 10
	 Train time/sample: 2560
	 Train time/batch_in_epoch: 10
	 Train time/sample_in_epoch: 2560
	 Train time/token: 5242880
	 Train time/token_in_epoch: 5242880
	 Train memory/current_allocated_mem: 12.1360
	 Train memory/current_active_mem: 12.1360
	 Train memory/current_inactive_mem: 3.2911
	 Train memory/current_reserved_mem: 41.3750
	 Train memory/peak_allocated_mem: 25.7980
	 Train memory/peak_active_mem: 26.6200
	 Train memory/peak_inactive_mem: 11.6470
	 Train memory/peak_reserved_mem: 41.3750
	 Train memory/alloc_retries: 0
	 Train trainer/device_train_microbatch_size: 16
	 Train loss/train/total: 11.4082
	 Train metrics/train/LanguageCrossEntropy: 11.4082
	 Train metrics/train/LanguagePerplexity: 90060.8906
	 Train throughput/batches_per_sec: 0.0701
	 Train throughput/samples_per_sec: 17.9539
	 Train throughput/device/batches_per_sec: 0.0044
	 Train throughput/device/samples_per_sec: 1.1221

# BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD
	 Train throughput/tokens_per_sec: 36769.6582
# BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD

	 Train throughput/device/tokens_per_sec: 2298.1036
	 Train throughput/flops_per_sec: 1587507178840928.7500
	 Train throughput/device/flops_per_sec: 99219198677558.0469
	 Train throughput/device/mfu: 0.3180
	 Train time/train: 0.0456
	 Train time/val: 0.0000
	 Train time/total: 0.0456
	 Train lr-DecoupledAdamW/group0: 0.0000
	 Train time/remaining_estimate: 0.3525
